"""HuggingFace Hub client for dataset upload."""

from __future__ import annotations

import asyncio
from pathlib import Path
from functools import partial

from loguru import logger


class HFClient:
    """Async wrapper around huggingface_hub for pushing datasets."""

    def __init__(self, token: str | None = None) -> None:
        from huggingface_hub import HfApi
        self._token = token
        self._api = HfApi(token=token)

    async def push_dataset(
        self,
        file_path: str | Path,
        repo_id: str,
        private: bool = False,
    ) -> dict:
        """Upload a dataset file to HuggingFace Hub.

        Creates the repo if it doesn't exist, uploads the data file and a README.
        Returns dict with repo_id, url, and files_uploaded count.
        """
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"Export file not found: {file_path}")

        loop = asyncio.get_event_loop()

        # Create repo (no-op if exists)
        await loop.run_in_executor(
            None,
            partial(
                self._api.create_repo,
                repo_id=repo_id,
                repo_type="dataset",
                private=private,
                exist_ok=True,
            ),
        )
        logger.info(f"Ensured HF repo exists: {repo_id}")

        files_uploaded = 0

        # Upload data file
        await loop.run_in_executor(
            None,
            partial(
                self._api.upload_file,
                path_or_fileobj=str(file_path),
                path_in_repo=f"data/{file_path.name}",
                repo_id=repo_id,
                repo_type="dataset",
            ),
        )
        files_uploaded += 1
        logger.info(f"Uploaded {file_path.name} to {repo_id}")

        # Generate and upload README (dataset card)
        readme_content = self._generate_dataset_card(repo_id, file_path)
        await loop.run_in_executor(
            None,
            partial(
                self._api.upload_file,
                path_or_fileobj=readme_content.encode("utf-8"),
                path_in_repo="README.md",
                repo_id=repo_id,
                repo_type="dataset",
            ),
        )
        files_uploaded += 1

        url = f"https://huggingface.co/datasets/{repo_id}"
        logger.info(f"Dataset pushed to {url}")

        return {
            "repo_id": repo_id,
            "url": url,
            "files_uploaded": files_uploaded,
        }

    @staticmethod
    def _generate_dataset_card(repo_id: str, file_path: Path) -> str:
        """Generate a minimal dataset card README."""
        name = repo_id.split("/")[-1] if "/" in repo_id else repo_id
        return f"""---
license: mit
task_categories:
  - text-generation
tags:
  - ai-data-factory
  - synthetic
---

# {name}

Dataset generated by [AI Data Factory](https://github.com/ai-data-factory).

## Files

- `data/{file_path.name}` --- Training examples in {file_path.suffix.lstrip('.')} format

## Generation

This dataset was automatically generated using the AI Data Factory pipeline:
1. Web content scraped and cleaned
2. Text chunked and deduplicated
3. Training examples generated via LLM
4. Quality-checked (toxicity, readability, format)
5. Exported and uploaded to HuggingFace Hub
"""
